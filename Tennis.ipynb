{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "from ddpg_multi_agents import Multi_Agents, GaussianNoise, OUNoise\n",
    "import os\n",
    "import json\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "agents_count = states.shape[0]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solving the environment : find best distribution of hyper parameters\n",
    "\n",
    "The Tennis environment is solved using a DDPG (Deep Determinic Policy Gradient). Common reinforcement learning techniques as DQN could not be applied here since it only apply to discrete action spaces in the sense that it searches for the action that maximize the action value function at each iteration.\n",
    "\n",
    "The DDPG algorithm introduces many parameters we can play on to speed up the convergence and the final score of the trained agent :\n",
    "\n",
    "- **buffer_size** : The size of the replay buffer in the context of the *Experience replay*,\n",
    "- **batch_size** : The size of the batch involved in the update enhanced using *Experience replay*,\n",
    "- **gamma_start** : The discount factor involved in the rewards expectation at the begining of the training,\n",
    "- **gamma_end** : The maximum discount factor,\n",
    "- **gamma_decay** : The discount factor increase coefficient at each iteration,\n",
    "- **(fc1, fc2)** : the size of the first and the second layer of the fully connected neural networks that are used for the actor and for the critic.\n",
    "- **lr_actor** : the learning rate of the neural netowrk of the actor,\n",
    "- **lr_critic** : the learning rate of the neural netowrk of the critic,\n",
    "- **noise** : the noise applied on the actor output to ease exploration, we test different parameterizations of the Ornstein-Uhlenbeck process suggested in the course but also test different gaussian noises.\n",
    "- **noise_factor_end** : the minimum coefficient applied to the noise factor (starting at 1 at the beginning of the training)\n",
    "- **noise_factor_decay** : the decay coefficient applied to the noise factor \n",
    "\n",
    "### 3.1 Define hyper parameter values to test\n",
    "\n",
    "The following cell introduces all the values we test for the hyper parameters described above :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values to test for each parameter\n",
    "hyper_parameters = {\n",
    "        'inbalanced_replay_memory_positive_reward_ratio' : [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "        'lr_actor, lr_critic' : [(1e-4, 1e-4), (1e-4, 1e-3), (1e-3, 1e-4), (1e-3, 1e-3)],\n",
    "        '(fc1,fc2)' : [(128, 128), (128, 256), (256, 128), (256, 512), (512, 256)],\n",
    "        'buffer_size':[int(1e5), int(1e6), int(1e7)],\n",
    "        'batch_size':[512, 1024, 2048, 4096],\n",
    "        '(gamma_start, gamma_end, gamma_increase)':[\n",
    "            (0.9, 0.99, 1.001),\n",
    "            (0.95, 0.995, 1.001),\n",
    "            (0.99, 0.999, 1.001),\n",
    "            (0.995, 0.9995, 1.001)\n",
    "        ],\n",
    "        '(tau_start, tau_end, tau_decay)':[\n",
    "            (0.005, 0.0005, 0.999),\n",
    "            (0.01, 0.001, 0.999),\n",
    "            (0.05, 0.005, 0.999)\n",
    "        ],\n",
    "        'noise' : [\n",
    "                   GaussianNoise(action_size, 0.1),\n",
    "                   GaussianNoise(action_size, 0.5),\n",
    "                   GaussianNoise(action_size, 1.0),\n",
    "                   OUNoise(action_size, 0, 0., 0.075, 0.1),\n",
    "                   OUNoise(action_size, 0, 0., 0.15, 0.2),\n",
    "                   OUNoise(action_size, 0, 0., 0.3, 0.4)\n",
    "                  ],\n",
    "        'noise_factor_decay':[0.9, 0.99, 0.999],\n",
    "        '(update_rate, update_count)' :[(1, 1), (10, 5), (5, 10), (10, 10), (10, 20), (20, 10), (20, 20)]\n",
    "    }\n",
    "\n",
    "default_parameters = {\n",
    "        'lr_actor, lr_critic' : (1e-4, 1e-4),\n",
    "        '(fc1,fc2)' :  (256, 256),\n",
    "        'buffer_size':int(1e5),\n",
    "        'batch_size':1024,\n",
    "        '(gamma_start, gamma_end, gamma_increase)' : (0.95, 0.995, 1.001),\n",
    "        '(tau_start, tau_end, tau_decay)' : (0.01, 0.001, 0.999),\n",
    "        #'noise' : OUNoise(action_size, 0, 0., 0.15, 0.2)\n",
    "         'noise' : GaussianNoise(action_size, 0.5),\n",
    "         'noise_factor_end' : 0.01,\n",
    "         'noise_factor_decay' : 0.99,\n",
    "         '(update_rate, update_count)' : (20, 20),\n",
    "         'inbalanced_replay_memory_positive_reward_ratio' : 0\n",
    "    }\n",
    " \n",
    "def to_name(parameters) :\n",
    "    \"\"\"\n",
    "    built a user friendly string that describes a particular experiment,\n",
    "    specified by a value distribution for parameters\n",
    "    \"\"\"\n",
    "    return ' '.join('{}={}'.format(name, str(value)) for name, value in parameters.items())\n",
    "\n",
    "\n",
    "def parameters_to_agent(parameters) :\n",
    "    buffer_size = parameters['buffer_size']\n",
    "    batch_size = parameters['batch_size']\n",
    "    fc1_units, fc2_units = parameters['(fc1,fc2)']\n",
    "    lr_actor, lr_critic = parameters['lr_actor, lr_critic']\n",
    "    noise = parameters['noise']\n",
    "    lr_actor, lr_critic = parameters['lr_actor, lr_critic']\n",
    "    update_rate, update_count = parameters['(update_rate, update_count)']\n",
    "    inbalanced_replay_memory_positive_reward_ratio = parameters['inbalanced_replay_memory_positive_reward_ratio']\n",
    "    return Multi_Agents(\n",
    "        agents_count,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        0,\n",
    "        buffer_size,\n",
    "        batch_size,\n",
    "        fc1_units,\n",
    "        fc2_units,\n",
    "        noise,\n",
    "        lr_actor,\n",
    "        lr_critic,\n",
    "        update_rate,\n",
    "        update_count, \n",
    "        inbalanced_replay_memory_positive_reward_ratio\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 A stop and resume implementation for hyper-parameterization : Save partial result\n",
    "Our hyper parameters tuning heuristic implies to training many times the agent. Since this is time consuming, we save the result after each training to be able to stop and resume our computations when we want without loosing what has already been computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load what has already been computed\n",
    "if os.path.exists('hyper_parameters_tuning.json'):\n",
    "    with open('hyper_parameters_tuning.json') as json_file:\n",
    "        results = json.load(json_file)\n",
    "else :\n",
    "    results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 training routine\n",
    "The following **ddpg** routine run the specified number of episodes per agent with the specified number of timeframe per episode. It is very similar to the implementation provided in the Udacity course (for instance [ddpg-pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum)) except that data like *actions*, *states*, *next_states*, *rewards*, *dones* are no longer single values but arrays (one value per agent).\n",
    "\n",
    "Since we implemented an hyper parameters tuning, we now receive the hyper parameters of the algorithm in the routine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ddpg(parameters,name,n_episodes=2000, max_t=10000000, train_mode = True, agent = None):\n",
    "    if agent is None :\n",
    "        agent = parameters_to_agent(parameters)\n",
    "        \n",
    "    gamma, gamma_end, gamma_increase = parameters['(gamma_start, gamma_end, gamma_increase)']\n",
    "    tau, tau_end, tau_decay = parameters['(tau_start, tau_end, tau_decay)']\n",
    "    noise_factor = 1.0\n",
    "    noise_factor_decay = parameters['noise_factor_decay']\n",
    "    noise_factor_end = parameters['noise_factor_end']\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name] \n",
    "        states = env_info.vector_observations\n",
    "        score = np.zeros(agents_count)\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, True, noise_factor)\n",
    "            env_info = env.step(actions)[brain_name] \n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones, gamma, tau)\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        #print(f'score = {score}')\n",
    "        scores.append(np.max(score))\n",
    "        scores_deque.append(np.max(score))\n",
    "        print(f'Episode {i_episode}\\t\\\n",
    "        score = {np.max(score)},\\\n",
    "        mean prev 100 = {np.mean(scores_deque)},\\\n",
    "        gamma = {gamma},\\\n",
    "        tau={tau}, noise_factor={noise_factor}\\\n",
    "        #positive_rewards = {len(agent.memory.positive_reward_memory) if agent.memory.inbalanced else \"-\"},\\\n",
    "        #negative rewards = {len(agent.memory.negative_reward_memory) if agent.memory.inbalanced else \"-\"},\\\n",
    "        ', end='\\r')\n",
    "        \n",
    "        \n",
    "        if train_mode :\n",
    "            torch.save(agent.actor_local.state_dict(), f'checkpoint_actor({name}).pth')\n",
    "            torch.save(agent.critic_local.state_dict(), f'checkpoint_critic({name}).pth')\n",
    "        agent.reset()  \n",
    "        \n",
    "        gamma = min(gamma_end, gamma*gamma_increase)\n",
    "        tau = max(tau_end, tau*tau_decay)\n",
    "        noise_factor = max(noise_factor_end, noise_factor*noise_factor_decay)\n",
    "    return agent, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Evaluate each value for each hyper parameter\n",
    "Train the agent for 200 iterations and save a json that gives for each experiment the value at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parameter inbalanced_replay_memory_positive_reward_ratio, value = 0.01 already tuned\n",
      "\n",
      "parameter inbalanced_replay_memory_positive_reward_ratio, value = 0.05 already tuned\n",
      "\n",
      "parameter inbalanced_replay_memory_positive_reward_ratio, value = 0.1 already tuned\n",
      "\n",
      "parameter inbalanced_replay_memory_positive_reward_ratio, value = 0.25 already tuned\n",
      "\n",
      "parameter inbalanced_replay_memory_positive_reward_ratio, value = 0.5 already tuned\n",
      "\n",
      "parameter inbalanced_replay_memory_positive_reward_ratio, value = 0.75 already tuned\n",
      "\n",
      "parameter inbalanced_replay_memory_positive_reward_ratio, value = 0.9 already tuned\n",
      "\n",
      "parameter lr_actor, lr_critic, value = (0.0001, 0.0001) already tuned\n",
      "\n",
      "parameter lr_actor, lr_critic, value = (0.0001, 0.001) already tuned\n",
      "\n",
      "parameter lr_actor, lr_critic, value = (0.001, 0.0001) already tuned\n",
      "\n",
      "parameter lr_actor, lr_critic, value = (0.001, 0.001) already tuned\n",
      "\n",
      "parameter (fc1,fc2), value = (128, 128) already tuned\n",
      "\n",
      "parameter (fc1,fc2), value = (128, 256) already tuned\n",
      "\n",
      "parameter (fc1,fc2), value = (256, 128) already tuned\n",
      "\n",
      "parameter (fc1,fc2), value = (256, 512) already tuned\n",
      "\n",
      "parameter (fc1,fc2), value = (512, 256) already tuned\n",
      "\n",
      "parameter buffer_size, value = 100000 already tuned\n",
      "\n",
      "parameter buffer_size, value = 1000000 already tuned\n",
      "\n",
      "parameter buffer_size, value = 10000000 already tuned\n",
      "\n",
      "parameter batch_size, value = 512 already tuned\n",
      "\n",
      "parameter batch_size, value = 1024 already tuned\n",
      "\n",
      "parameter batch_size, value = 2048 already tuned\n",
      "\n",
      "parameter batch_size, value = 4096 already tuned\n",
      "\n",
      "parameter (gamma_start, gamma_end, gamma_increase), value = (0.9, 0.99, 1.001) already tuned\n",
      "\n",
      "parameter (gamma_start, gamma_end, gamma_increase), value = (0.95, 0.995, 1.001) already tuned\n",
      "\n",
      "parameter (gamma_start, gamma_end, gamma_increase), value = (0.99, 0.999, 1.001) already tuned\n",
      "\n",
      "parameter (gamma_start, gamma_end, gamma_increase), value = (0.995, 0.9995, 1.001) already tuned\n",
      "\n",
      "parameter (tau_start, tau_end, tau_decay), value = (0.005, 0.0005, 0.999) already tuned\n",
      "\n",
      "parameter (tau_start, tau_end, tau_decay), value = (0.01, 0.001, 0.999) already tuned\n",
      "\n",
      "parameter (tau_start, tau_end, tau_decay), value = (0.05, 0.005, 0.999) already tuned\n",
      "\n",
      "parameter noise, value = GaussianNoise(factor = 0.1) already tuned\n",
      "\n",
      "parameter noise, value = GaussianNoise(factor = 0.5) already tuned\n",
      "\n",
      "parameter noise, value = GaussianNoise(factor = 1.0) already tuned\n",
      "\n",
      "parameter noise, value = OUNoise(mu = 0.0 ,theta = 0.075, sigma = 0.1) already tuned\n",
      "\n",
      "parameter noise, value = OUNoise(mu = 0.0 ,theta = 0.15, sigma = 0.2) already tuned\n",
      "\n",
      "parameter noise, value = OUNoise(mu = 0.0 ,theta = 0.3, sigma = 0.4) already tuned\n",
      "\n",
      "parameter noise_factor_decay, value = 0.9 already tuned\n",
      "\n",
      "parameter noise_factor_decay, value = 0.99 already tuned\n",
      "\n",
      "parameter noise_factor_decay, value = 0.999 already tuned\n",
      "\n",
      "parameter (update_rate, update_count), value = (1, 1) already tuned\n",
      "\n",
      "parameter (update_rate, update_count), value = (10, 5) already tuned\n",
      "\n",
      "parameter (update_rate, update_count), value = (5, 10) already tuned\n",
      "\n",
      "parameter (update_rate, update_count), value = (10, 10) already tuned\n",
      "\n",
      "parameter (update_rate, update_count), value = (10, 20) already tuned\n",
      "\n",
      "parameter (update_rate, update_count), value = (20, 10) already tuned\n",
      "\n",
      "parameter (update_rate, update_count), value = (20, 20) already tuned\n"
     ]
    }
   ],
   "source": [
    "# find sequentially the \"best\" value for each parameters after 200 episodes. \n",
    "# When a parameter is choosen, the others take values from the dqn project\n",
    "\n",
    "for name, parameter in hyper_parameters.items() :\n",
    "    \n",
    "    for value in parameter :\n",
    "        key = '{}_{}'.format(name, str(value))\n",
    "        if not key in results :\n",
    "            print(\"\\n\\nTuning parameter {}, value = {}\".format(name, value))\n",
    "            parameters = default_parameters.copy()\n",
    "            parameters[name] = value\n",
    "            print(parameters)\n",
    "            _, scores = ddpg(parameters, name, 2000)\n",
    "            results[key] = scores\n",
    "            with open('hyper_parameters_tuning.json', 'w') as fp:\n",
    "                json.dump(results, fp)       \n",
    "        else :\n",
    "            print(\"\\nparameter {}, value = {} already tuned\".format(name, value))\n",
    "with open('hyper_parameters_tuning.json', 'w') as fp:\n",
    "    json.dump(results, fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solving the environment : Train the model with the best hyper parameters values find before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(positive_reward_memory) = 10000, len(negative_reward_memory) = 990000\n",
      "Episode 2000\t        score = 1.9000000283122063,        mean prev 100 = 2.0418000304326416,        gamma = 0.995,        tau=0.0006766763032907868, noise_factor=0.01        #positive_rewards = 48147,        #negative rewards = 1000000,         02,                \r"
     ]
    }
   ],
   "source": [
    "chosen_parameters = {\n",
    "        'lr_actor, lr_critic' : (1e-4, 1e-4),\n",
    "        '(fc1,fc2)' :  (256, 512),\n",
    "        'buffer_size':int(1e6),\n",
    "        'batch_size':1024,\n",
    "        '(gamma_start, gamma_end, gamma_increase)' : (0.95, 0.995, 1.001),\n",
    "        '(tau_start, tau_end, tau_decay)' : (0.005, 0.0005, 0.999),\n",
    "         'noise' : OUNoise(action_size, 0, 0., 0.3, 0.4),\n",
    "         'noise_factor_end' : 0.01,\n",
    "         'noise_factor_decay' : 0.99,\n",
    "         '(update_rate, update_count)' : (10, 20),\n",
    "         'inbalanced_replay_memory_positive_reward_ratio' : 0.01\n",
    "    }\n",
    "agent, scores = ddpg(chosen_parameters, 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the scores to show that it is above 0.5 for more than 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = {}\n",
    "final_scores['best'] = scores\n",
    "with open('final_scores.json', 'w') as fp:\n",
    "    json.dump(final_scores, fp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "scores = list(scores)\n",
    "# plot the score\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
